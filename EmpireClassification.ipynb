{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf46f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC as SupportVectorClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Concatenate, Embedding, Input, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from collections import deque, Counter\n",
    "from itertools import permutations, repeat, combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0429344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empires = []\n",
    "vectorizer = CountVectorizer()\n",
    "min_sentence_length = 5\n",
    "level_sample_sizes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d4217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Resources/Data/EmpireText.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    line = file.readline().replace(\"\\n\", \"\").split(\" \")\n",
    "    for empire in line:\n",
    "        empires.append(empire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6852b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "labels = []\n",
    "with open(\"Resources/Data/EmpireText.txt\", \"r\", encoding=\"utf8\") as file: \n",
    "    for line in file.readlines()[1:]:\n",
    "        line = line.strip().replace(\"\\n\", \"\")\n",
    "        line = re.sub(\"\\[.{0,4}]\", \"\", line) #remove wikipedia citings\n",
    "\n",
    "\n",
    "        if line in empires:\n",
    "            current_empire = line\n",
    "            continue\n",
    "        elif len(line.replace(\" \", \"\")) < 15:\n",
    "            continue\n",
    "        elif line[-1] != \".\":\n",
    "            line += \".\"\n",
    "    \n",
    "        line = nltk.sent_tokenize(line)\n",
    "        for sentence in line:\n",
    "            text.append(sentence)\n",
    "            labels.append(current_empire)\n",
    "            \n",
    "data = pd.DataFrame({\"label\":labels, \"text\":text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6bdd19",
   "metadata": {},
   "source": [
    "# Paragraphing the text\n",
    "Because the data came in paragraphs, many sentences have no coorelation with any empire, because of this, the models will not be able to a correct conclusion for these sentences. Combining several sentences into a paragraph, and using that paragraph will allow for a model to overcome this obstacle. It will also give us more training samples since were taking the permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0a5a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[        label                                               text\n",
      "1     Ottoman  Known as one of history’s most powerful empire...\n",
      "1311  Ottoman  The empire’s success lay in its centralized st...\n",
      "1312  Ottoman  But all empires that rise must fall, and six c...\n",
      "1313  Ottoman  Osman I, a leader of a nomadic Turkic tribe fr...\n",
      "1314  Ottoman  Around 1299, he declared himself supreme leade...\n",
      "...       ...                                                ...\n",
      "2253  Ottoman  In the 19th century, Ishak Efendi is credited ...\n",
      "2254  Ottoman  The main sports Ottomans were engaged in were ...\n",
      "2255  Ottoman  European model sports clubs were formed with t...\n",
      "2256  Ottoman  The leading clubs, according to timeline, were...\n",
      "2257  Ottoman  Football clubs were formed in other provinces ...\n",
      "\n",
      "[948 rows x 2 columns],       label                                               text\n",
      "0     Roman  The Roman Empire, the ancient empire, centred ...\n",
      "4     Roman  A period of unrest and civil wars in the 1st c...\n",
      "5     Roman  This period encompassed the career of Julius C...\n",
      "6     Roman  After his assassination in 44 BCE, the triumvi...\n",
      "7     Roman  It was not long before Octavian went to war ag...\n",
      "...     ...                                                ...\n",
      "1306  Roman  The western half went into decline, gradually ...\n",
      "1307  Roman  An assortment of groups including the Goths, V...\n",
      "1308  Roman  Ancient Rome was sacked twice, first by the Go...\n",
      "1309  Roman  In A.D. 476 the Western Roman Empire officiall...\n",
      "1310  Roman  But the eastern half, based at Constantinople,...\n",
      "\n",
      "[1308 rows x 2 columns],         label                                               text\n",
      "3     Russian  The Russian Empire, also known as Imperial Rus...\n",
      "3194  Russian  The rise of the Russian Empire coincided with ...\n",
      "3195  Russian  The Empire lasted until the Republic was procl...\n",
      "3196  Russian  The third-largest empire in history, at one po...\n",
      "3197  Russian  It had 125.6 million subjects according to the...\n",
      "...       ...                                                ...\n",
      "4181  Russian  Of technical inventions, perhaps Paul Yablochk...\n",
      "4182  Russian  Yablochkov developed the electric light before...\n",
      "4183  Russian  In spite of these achievements, Russian scient...\n",
      "4184  Russian  European, and to a lesser extent, Americans we...\n",
      "4185  Russian  Although it made steady progress in science an...\n",
      "\n",
      "[993 rows x 2 columns],         label                                               text\n",
      "2     Spanish  The Spanish Empire (Spanish: Imperio español),...\n",
      "2258  Spanish  One of the largest empires in history, it was,...\n",
      "2259  Spanish  It was one of the most powerful empires of the...\n",
      "2260  Spanish  An important element in the formation of Spain...\n",
      "2261  Spanish  Castile became the dominant kingdom in Iberia ...\n",
      "...       ...                                                ...\n",
      "3189  Spanish  The Spanish navy was expanded and commerce wit...\n",
      "3190  Spanish  Unfortunately, their heavy favouritism of peni...\n",
      "3191  Spanish  Despite the best efforts of Charles III, Spain...\n",
      "3192  Spanish  Napoleon named his brother Joseph as the king ...\n",
      "3193  Spanish  At first, the colonials upheld the rights of F...\n",
      "\n",
      "[937 rows x 2 columns]]\n"
     ]
    }
   ],
   "source": [
    "dataframes = [df for _, df in data.groupby('label')]\n",
    "print(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c0a9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "labels = []\n",
    "for df in dataframes:\n",
    "    label = df['label'].tolist()[0]\n",
    "    sentences = df['text']\n",
    "    paragraph_permutations = list(permutations(sentences, 2))\n",
    "    paragraphs.extend(paragraph_permutations)\n",
    "    labels.extend(list(repeat(label, len(paragraph_permutations))))\n",
    "\n",
    "data = pd.DataFrame({'label':labels, 'text': paragraphs})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e0173",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a73ab61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>(Known as one of history’s most powerful empir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>(Known as one of history’s most powerful empir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>(Known as one of history’s most powerful empir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>(Known as one of history’s most powerful empir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>(Known as one of history’s most powerful empir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0  Ottoman  (Known as one of history’s most powerful empir...\n",
       "1  Ottoman  (Known as one of history’s most powerful empir...\n",
       "2  Ottoman  (Known as one of history’s most powerful empir...\n",
       "3  Ottoman  (Known as one of history’s most powerful empir...\n",
       "4  Ottoman  (Known as one of history’s most powerful empir..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a20d9cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4469400 entries, 0 to 4469399\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   label   object\n",
      " 1   text    object\n",
      "dtypes: object(2)\n",
      "memory usage: 68.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54319e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Roman      1709556\n",
       "Russian     985056\n",
       "Ottoman     897756\n",
       "Spanish     877032\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts = data[\"label\"].value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d156b",
   "metadata": {},
   "source": [
    "# Text classification:\n",
    "### The process of text classification is comprised of 4 main steps\n",
    "#### - Preprocessing the text\n",
    "#### - Encoding labels\n",
    "#### - Vectorizing the text\n",
    "#### - Training the model(s)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562a889",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "### Steps:\n",
    "- Lowercasing\n",
    "- Tokenization\n",
    "- POS tagging\n",
    "- Lemmatization\n",
    "\n",
    "Note: When tested, the models showed better results without the removal of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bdb28",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "599815ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Known as one of history’s most powerful empires, the Ottoman Empire grew from a Turkish stronghold in Anatolia into a vast state that at its peak reached as far north as Vienna, Austria, as far east as the Persian Gulf, as far west as Algeria, and as far south as Yemen. The city named for Constantine, the first Christian emperor of Rome, then also became known as Istanbul (a version of stin polis, Greek for “in the city” or “to the city.”.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_paragraph = data['text'][5]\n",
    "example_paragraph = \" \".join(example_paragraph)\n",
    "example_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2fc66e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'known as one of history’s most powerful empires, the ottoman empire grew from a turkish stronghold in anatolia into a vast state that at its peak reached as far north as vienna, austria, as far east as the persian gulf, as far west as algeria, and as far south as yemen. the city named for constantine, the first christian emperor of rome, then also became known as istanbul (a version of stin polis, greek for “in the city” or “to the city.”.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase_paragraph = example_paragraph.lower()\n",
    "lowercase_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfccf56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'known', 'as', 'one', 'of', 'history', '’', 's', 'most', 'powerful', 'empires', ',', 'the', 'ottoman', 'empire', 'grew', 'from', 'a', 'turkish', 'stronghold', 'in', 'anatolia', 'into', 'a', 'vast', 'state', 'that', 'at', 'its', 'peak', 'reached', 'as', 'far', 'north', 'as', 'vienna', ',', 'austria', ',', 'as', 'far', 'east', 'as', 'the', 'persian', 'gulf', ',', 'as', 'far', 'west', 'as', 'algeria', ',', 'and', 'as', 'far', 'south', 'as', 'yemen', '.', 'the', 'city', 'named', 'for', 'constantine', ',', 'the', 'first', 'christian', 'emperor', 'of', 'rome', ',', 'then', 'also', 'became', 'known', 'as', 'istanbul', '(', 'a', 'version', 'of', 'stin', 'polis', ',', 'greek', 'for', '“', 'in', 'the', 'city', '”', 'or', '“', 'to', 'the', 'city.', '”', '.', "
     ]
    }
   ],
   "source": [
    "tokenized_paragraph = nltk.word_tokenize(lowercase_paragraph)\n",
    "for word in tokenized_paragraph:\n",
    "    print(f\"'{word}'\", end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48bd2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('known', 'VBN'), ('as', 'IN'), ('one', 'CD'), ('of', 'IN'), ('history', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('most', 'RBS'), ('powerful', 'JJ'), ('empires', 'NNS'), (',', ','), ('the', 'DT'), ('ottoman', 'NN'), ('empire', 'NN'), ('grew', 'VBD'), ('from', 'IN'), ('a', 'DT'), ('turkish', 'JJ'), ('stronghold', 'NN'), ('in', 'IN'), ('anatolia', 'NN'), ('into', 'IN'), ('a', 'DT'), ('vast', 'JJ'), ('state', 'NN'), ('that', 'WDT'), ('at', 'IN'), ('its', 'PRP$'), ('peak', 'NN'), ('reached', 'VBN'), ('as', 'IN'), ('far', 'RB'), ('north', 'JJ'), ('as', 'IN'), ('vienna', 'NN'), (',', ','), ('austria', 'RB'), (',', ','), ('as', 'IN'), ('far', 'RB'), ('east', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('persian', 'JJ'), ('gulf', 'NN'), (',', ','), ('as', 'IN'), ('far', 'RB'), ('west', 'JJ'), ('as', 'IN'), ('algeria', 'NNS'), (',', ','), ('and', 'CC'), ('as', 'IN'), ('far', 'RB'), ('south', 'JJ'), ('as', 'IN'), ('yemen', 'NNS'), ('.', '.'), ('the', 'DT'), ('city', 'NN'), ('named', 'VBN'), ('for', 'IN'), ('constantine', 'NN'), (',', ','), ('the', 'DT'), ('first', 'JJ'), ('christian', 'JJ'), ('emperor', 'NN'), ('of', 'IN'), ('rome', 'NN'), (',', ','), ('then', 'RB'), ('also', 'RB'), ('became', 'VBD'), ('known', 'VBN'), ('as', 'IN'), ('istanbul', 'NN'), ('(', '('), ('a', 'DT'), ('version', 'NN'), ('of', 'IN'), ('stin', 'NN'), ('polis', 'NN'), (',', ','), ('greek', 'NN'), ('for', 'IN'), ('“', 'NN'), ('in', 'IN'), ('the', 'DT'), ('city', 'NN'), ('”', 'NN'), ('or', 'CC'), ('“', 'NN'), ('to', 'TO'), ('the', 'DT'), ('city.', 'NN'), ('”', 'NN'), ('.', '.'), "
     ]
    }
   ],
   "source": [
    "tagged_paragraph = nltk.pos_tag(tokenized_paragraph)\n",
    "for word in tagged_paragraph:\n",
    "    print(word, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41ddf9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'know', 'a', 'one', 'of', 'history', '’', 's', 'most', 'powerful', 'empire', ',', 'the', 'ottoman', 'empire', 'grow', 'from', 'a', 'turkish', 'stronghold', 'in', 'anatolia', 'into', 'a', 'vast', 'state', 'that', 'at', 'it', 'peak', 'reach', 'a', 'far', 'north', 'a', 'vienna', ',', 'austria', ',', 'a', 'far', 'east', 'a', 'the', 'persian', 'gulf', ',', 'a', 'far', 'west', 'a', 'algeria', ',', 'and', 'a', 'far', 'south', 'a', 'yemen', '.', 'the', 'city', 'name', 'for', 'constantine', ',', 'the', 'first', 'christian', 'emperor', 'of', 'rome', ',', 'then', 'also', 'become', 'know', 'a', 'istanbul', '(', 'a', 'version', 'of', 'stin', 'polis', ',', 'greek', 'for', '“', 'in', 'the', 'city', '”', 'or', '“', 'to', 'the', 'city.', '”', '.', "
     ]
    }
   ],
   "source": [
    "def get_pos(tag):    \n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_sentence = [wnl.lemmatize(word, get_pos(pos)) for word, pos in tagged_paragraph]\n",
    "for word in lemmatized_sentence:\n",
    "    print(f\"'{word}'\", end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931ffa2",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e009df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "wnl = WordNetLemmatizer()\n",
    "for row in data.iterrows():\n",
    "    label = row[1]['label']\n",
    "    text = row[1]['text']\n",
    "    text = \" \".join(text)\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = nltk.pos_tag(text)\n",
    "    text = [wnl.lemmatize(word, get_pos(pos)) for word, pos in text]\n",
    "    sentences.append(text)\n",
    "    labels.append(label)\n",
    "\n",
    "data = pd.DataFrame({\"label\":labels, \"text\":sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fc53e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ottoman</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0  Ottoman  [Known, a, one, of, history, ’, s, most, power...\n",
       "1  Ottoman  [Known, a, one, of, history, ’, s, most, power...\n",
       "2  Ottoman  [Known, a, one, of, history, ’, s, most, power...\n",
       "3  Ottoman  [Known, a, one, of, history, ’, s, most, power...\n",
       "4  Ottoman  [Known, a, one, of, history, ’, s, most, power..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16fa89",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f74b9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = []\n",
    "for label in data['label']:\n",
    "    labels.append(empires.index(label))\n",
    "data['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f1fcf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[Known, a, one, of, history, ’, s, most, power...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  [Known, a, one, of, history, ’, s, most, power...\n",
       "1      1  [Known, a, one, of, history, ’, s, most, power...\n",
       "2      1  [Known, a, one, of, history, ’, s, most, power...\n",
       "3      1  [Known, a, one, of, history, ’, s, most, power...\n",
       "4      1  [Known, a, one, of, history, ’, s, most, power..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e3cca",
   "metadata": {},
   "source": [
    "## Extra step: Equalizing the number of samples\n",
    "Data with varied distribution will perform worse unless accounted for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "617b4f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    877032\n",
       "1    877032\n",
       "2    877032\n",
       "3    877032\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = Counter(data[\"label\"])\n",
    "max_len = min(label_counts.values())\n",
    "data = data.sample(frac=1).groupby('label').head(max_len)\n",
    "labels = data['label']\n",
    "text = data['text']\n",
    "value_counts = data['label'].value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09743410",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f925ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "text = []\n",
    "for sentence in data['text']:\n",
    "    text.append(\" \".join(sentence))\n",
    "vectorizer.fit(text)\n",
    "text = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af6efac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Jason\\Documents\\GitHub\\EmpireClassification\\EmpireClassification.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Jason/Documents/GitHub/EmpireClassification/EmpireClassification.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vectorized_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m:labels, \u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m:text})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Jason/Documents/GitHub/EmpireClassification/EmpireClassification.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vectorized_data\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py:636\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    630\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    631\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    634\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 636\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    637\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    638\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:502\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    494\u001b[0m     arrays \u001b[39m=\u001b[39m [\n\u001b[0;32m    495\u001b[0m         x\n\u001b[0;32m    496\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m    497\u001b[0m         \u001b[39melse\u001b[39;00m x\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    498\u001b[0m         \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays\n\u001b[0;32m    499\u001b[0m     ]\n\u001b[0;32m    500\u001b[0m     \u001b[39m# TODO: can we get rid of the dt64tz special case above?\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:125\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    122\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[0;32m    124\u001b[0m     \u001b[39m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     arrays \u001b[39m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[0;32m    126\u001b[0m     \u001b[39m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     \u001b[39m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[39m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:625\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    622\u001b[0m             val \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(val)\n\u001b[0;32m    623\u001b[0m         val \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_multiget(val, oindex\u001b[39m.\u001b[39m_values, default\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan)\n\u001b[1;32m--> 625\u001b[0m     val \u001b[39m=\u001b[39m sanitize_array(\n\u001b[0;32m    626\u001b[0m         val, index, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, raise_cast_failure\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m    627\u001b[0m     )\n\u001b[0;32m    628\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(val, index)\n\u001b[0;32m    630\u001b[0m homogenized\u001b[39m.\u001b[39mappend(val)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\construction.py:591\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[0;32m    589\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(data)\n\u001b[0;32m    593\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    594\u001b[0m     subarr \u001b[39m=\u001b[39m _try_cast(data, dtype, copy, raise_cast_failure)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\sparse\\_csr.py:244\u001b[0m, in \u001b[0;36mcsr_matrix.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i0:i1]\n\u001b[0;32m    243\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[i0:i1]\n\u001b[1;32m--> 244\u001b[0m \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m(\n\u001b[0;32m    245\u001b[0m     (data, indices, indptr), shape\u001b[39m=\u001b[39;49mshape, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    246\u001b[0m )\n\u001b[0;32m    247\u001b[0m i0 \u001b[39m=\u001b[39m i1\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\sparse\\_compressed.py:106\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 106\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_format(full_check\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\sparse\\_compressed.py:147\u001b[0m, in \u001b[0;36m_cs_matrix.check_format\u001b[1;34m(self, full_check)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39m# use _swap to determine proper bounds\u001b[39;00m\n\u001b[0;32m    146\u001b[0m major_name, minor_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap((\u001b[39m'\u001b[39m\u001b[39mrow\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m--> 147\u001b[0m major_dim, minor_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_swap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    149\u001b[0m \u001b[39m# index arrays should have integer data types\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mi\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorized_data = pd.DataFrame({\"label\":labels, \"text\":text})\n",
    "vectorized_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502c356",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f08fa455",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = set()\n",
    "for sentence in data['text']:\n",
    "    for word in sentence:\n",
    "        uniques.add(word)\n",
    "num_uniques = len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0af671e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(text, labels, test_size=.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0097ae",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a962d610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993956894413834"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=2, max_depth=150, max_features=1000)\n",
    "decision_tree.fit(x_train, y_train)\n",
    "decision_tree_score = decision_tree.score(x_test, y_test)\n",
    "decision_tree_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782ac3b",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0847116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0, n_estimators=300, max_depth=150, max_features=1000)\n",
    "random_forest.fit(x_train, y_train)\n",
    "random_forest_score = random_forest.score(x_test, y_test)\n",
    "random_forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a032c1",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9251f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c = 0.1\n",
    "best_kernel = \"linear\"\n",
    "support_vector_machine = SupportVectorClassifier(kernel=best_kernel, C=best_c)\n",
    "support_vector_machine.fit(x_train, y_train)\n",
    "support_vector_machine_score = support_vector_machine.score(x_test, y_test)\n",
    "support_vector_machine_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2560e5",
   "metadata": {},
   "source": [
    "### Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8895a",
   "metadata": {},
   "source": [
    "Sidenote: while one could convert x_Train from SparseTensor to array it causes the lstm model to have low accuracy and take 20 min per epoch, so its better to just format the data with tensorflow methods to convert it to a format that tensorflow is better equipped to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input example: [8438 6972 8143 2069 7777 9294 5225  427 3924  555 2612 7586 3226 9440\n",
      " 2612 9395 2686 6797 5225 8126 3924 1931  665 2069 6344 3226 7690    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "label example: [0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "text = data['text']\n",
    "labels = data['label']\n",
    "longest_sentence = max(text, key=lambda x: len(x))\n",
    "encoded_sentences = [one_hot(\" \".join(sentence), num_uniques) for sentence in text]\n",
    "padded_sequences = pad_sequences(encoded_sentences, maxlen=len(longest_sentence), padding='post')\n",
    "\n",
    "def encode_labels(labels):\n",
    "    output = []\n",
    "    for label in labels:\n",
    "        label_array = np.zeros(len(empires))\n",
    "        label_array[label] = 1    \n",
    "        output.append(label_array)\n",
    "    return output\n",
    "\n",
    "labels = encode_labels(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=.1, random_state=85)\n",
    "print(\"input example:\", x_train[0])\n",
    "print(\"label example:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1dcc00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_29 (Embedding)    (None, None, 32)          316192    \n",
      "                                                                 \n",
      " bidirectional_29 (Bidirecti  (None, 200)              106400    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 256)               51456     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 507,460\n",
      "Trainable params: 507,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(num_uniques, 32))\n",
    "lstm.add(Bidirectional(LSTM(100)))\n",
    "lstm.add(Dense(256, activation=\"relu\"))\n",
    "lstm.add(Dropout(0.5))\n",
    "lstm.add(Dense(128))\n",
    "lstm.add(Dense(4, activation=\"sigmoid\"))\n",
    "optimizer = Adam(learning_rate=0.03)\n",
    "lstm.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de356a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "28/28 [==============================] - 15s 536ms/step - loss: 0.1534 - accuracy: 0.9520\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 16s 585ms/step - loss: 0.0568 - accuracy: 0.9837\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 20s 711ms/step - loss: 0.0308 - accuracy: 0.9913\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 18s 634ms/step - loss: 0.0240 - accuracy: 0.9933\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 15s 523ms/step - loss: 0.0140 - accuracy: 0.9961\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 0.0183 - accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "lstm.fit(np.array(padded_sequences), np.array(labels), epochs=5, verbose=1, batch_size=128, callbacks=[])\n",
    "lstm_score = lstm.evaluate(x_test, np.array(y_test), verbose=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea783a6f",
   "metadata": {},
   "source": [
    "## Ensemble learning - Stacking\n",
    "Stacking is a technique that allows you to use the predictions of several machine learning models/algorithms by training a neural network to combine the ouputs of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cec07d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bf6f67fea791184c93f19e0544c57959b00b02fc54e678238e4386d05328beb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
