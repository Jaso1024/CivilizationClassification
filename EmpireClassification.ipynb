{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9cf46f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC as SupportVectorClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM, Concatenate, Embedding, Input, Bidirectional, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow_addons.activations import mish\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import contractions\n",
    "\n",
    "from collections import deque, Counter\n",
    "from itertools import permutations, repeat, combinations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0429344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "empires = []\n",
    "vectorizer = CountVectorizer()\n",
    "min_sentence_length = 7 # average length of a sentence is between 15 and 20 words\n",
    "level_sample_sizes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82d4217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Resources/Data/EmpireText.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    line = file.readline().replace(\"\\n\", \"\").split(\" \")\n",
    "    for empire in line:\n",
    "        empires.append(empire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e6852b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentence(sentence, current_empire):\n",
    "        if len(sentence.split(\" \")) > min_sentence_length:\n",
    "            sentence = contractions.fix(sentence)\n",
    "            contains_other_empire = False\n",
    "            for empire in empires:\n",
    "                if empire == current_empire:\n",
    "                    continue\n",
    "                elif re.search(empire.lower(), sentence.lower()) is not None:\n",
    "                    contains_other_empire = True\n",
    "\n",
    "            if not contains_other_empire:\n",
    "                return sentence.strip()\n",
    "\n",
    "text = []\n",
    "labels = []\n",
    "with open(\"Resources/Data/EmpireText.txt\", \"r\", encoding=\"utf8\") as file: \n",
    "    for line in file.readlines()[1:]:\n",
    "        line = line.strip().replace(\"\\n\", \"\")\n",
    "        line = re.sub(\"\\[.{0,4}]\", \"\", line) #remove wikipedia citings\n",
    "        line = line.replace(\"C.\", \"C\")\n",
    "        line = line.replace(\"E.\", \"E\")\n",
    "        line = line.replace(\"D.\", \"D\")\n",
    "        \n",
    "        if line in empires:\n",
    "            current_empire = line\n",
    "            continue\n",
    "        elif len(line.replace(\" \", \"\")) < min_sentence_length:\n",
    "            continue\n",
    "        elif len(line.split(\" \")) < min_sentence_length:\n",
    "            continue\n",
    "        elif line[-1] != \".\":\n",
    "            line += \".\"\n",
    "    \n",
    "        line = nltk.sent_tokenize(line)\n",
    "        formatted_line = \"\"\n",
    "        for sentence in line:\n",
    "            sentence = format_sentence(sentence, current_empire)\n",
    "            if sentence is not None:\n",
    "                formatted_line += sentence + \" \"\n",
    "        text.append(formatted_line)\n",
    "        labels.append(current_empire)\n",
    "            \n",
    "data = pd.DataFrame({\"label\":labels, \"text\":text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e0173",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32657aad",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3519ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roman</td>\n",
       "      <td>The Roman Empire, the ancient empire, centred ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>The Spanish Empire (Spanish: Imperio españold)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russian</td>\n",
       "      <td>The Russian Empire, also known as Imperial Rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roman</td>\n",
       "      <td>A period of unrest and civil wars in the 1st c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roman</td>\n",
       "      <td>Augustus established a form of government know...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0    Roman  The Roman Empire, the ancient empire, centred ...\n",
       "1  Spanish  The Spanish Empire (Spanish: Imperio españold)...\n",
       "2  Russian  The Russian Empire, also known as Imperial Rus...\n",
       "3    Roman  A period of unrest and civil wars in the 1st c...\n",
       "4    Roman  Augustus established a form of government know..."
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2a2a062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 696 entries, 0 to 695\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   696 non-null    object\n",
      " 1   text    696 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 11.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0567be82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Roman      319\n",
       "Spanish    228\n",
       "Russian    149\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_value_counts = data[\"label\"].value_counts()\n",
    "training_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d156b",
   "metadata": {},
   "source": [
    "# Text classification:\n",
    "### The process of text classification is comprised of 4 main steps\n",
    "#### - Preprocessing the text\n",
    "#### - Encoding labels\n",
    "#### - Vectorizing the text\n",
    "#### - Training the model(s)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562a889",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "### Steps:\n",
    "- Lowercasing\n",
    "- Tokenization\n",
    "- POS tagging\n",
    "- Lemmatization\n",
    "\n",
    "Note: When tested, the models showed better results without the removal of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96bdb28",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "599815ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish empire:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Spanish Empire (Spanish: Imperio españold), also known as the Hispanic Monarchy (Spanish: Monarquía Hispánica) or the Catholic Monarchy (Spanish: Monarquía Católica) was a colonial empire governed by Spain and its predecessor states between 1492 and 1976. '"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_paragraph = data['text'][1]\n",
    "example_paragraph = \"\".join(example_paragraph)\n",
    "print(data['label'][1], \"empire:\")\n",
    "example_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b2fc66e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the spanish empire (spanish: imperio españold), also known as the hispanic monarchy (spanish: monarquía hispánica) or the catholic monarchy (spanish: monarquía católica) was a colonial empire governed by spain and its predecessor states between 1492 and 1976. '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase_paragraph = example_paragraph.lower()\n",
    "lowercase_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dfccf56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the', 'spanish', 'empire', '(', 'spanish', ':', 'imperio', 'españold', ')', ',', 'also', 'known', 'as', 'the', 'hispanic', 'monarchy', '(', 'spanish', ':', 'monarquía', 'hispánica', ')', 'or', 'the', 'catholic', 'monarchy', '(', 'spanish', ':', 'monarquía', 'católica', ')', 'was', 'a', 'colonial', 'empire', 'governed', 'by', 'spain', 'and', 'its', 'predecessor', 'states', 'between', '1492', 'and', '1976', '.', "
     ]
    }
   ],
   "source": [
    "tokenized_paragraph = nltk.word_tokenize(lowercase_paragraph)\n",
    "for word in tokenized_paragraph:\n",
    "    print(f\"'{word}'\", end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48bd2b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'DT'), ('spanish', 'JJ'), ('empire', 'NN'), ('(', '('), ('spanish', 'JJ'), (':', ':'), ('imperio', 'NN'), ('españold', 'NN'), (')', ')'), (',', ','), ('also', 'RB'), ('known', 'VBN'), ('as', 'IN'), ('the', 'DT'), ('hispanic', 'JJ'), ('monarchy', 'NN'), ('(', '('), ('spanish', 'JJ'), (':', ':'), ('monarquía', 'NN'), ('hispánica', 'NN'), (')', ')'), ('or', 'CC'), ('the', 'DT'), ('catholic', 'JJ'), ('monarchy', 'NN'), ('(', '('), ('spanish', 'JJ'), (':', ':'), ('monarquía', 'NN'), ('católica', 'NN'), (')', ')'), ('was', 'VBD'), ('a', 'DT'), ('colonial', 'JJ'), ('empire', 'NN'), ('governed', 'VBN'), ('by', 'IN'), ('spain', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('predecessor', 'NN'), ('states', 'NNS'), ('between', 'IN'), ('1492', 'CD'), ('and', 'CC'), ('1976', 'CD'), ('.', '.'), "
     ]
    }
   ],
   "source": [
    "tagged_paragraph = nltk.pos_tag(tokenized_paragraph)\n",
    "for word in tagged_paragraph:\n",
    "    print(word, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41ddf9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the', 'spanish', 'empire', '(', 'spanish', ':', 'imperio', 'españold', ')', ',', 'also', 'know', 'a', 'the', 'hispanic', 'monarchy', '(', 'spanish', ':', 'monarquía', 'hispánica', ')', 'or', 'the', 'catholic', 'monarchy', '(', 'spanish', ':', 'monarquía', 'católica', ')', 'be', 'a', 'colonial', 'empire', 'govern', 'by', 'spain', 'and', 'it', 'predecessor', 'state', 'between', '1492', 'and', '1976', '.', "
     ]
    }
   ],
   "source": [
    "def get_pos(tag):    \n",
    "    if tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "        \n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_sentence = [wnl.lemmatize(word, get_pos(pos)) for word, pos in tagged_paragraph]\n",
    "for word in lemmatized_sentence:\n",
    "    print(f\"'{word}'\", end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931ffa2",
   "metadata": {},
   "source": [
    "### Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3e009df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "labels = []\n",
    "wnl = WordNetLemmatizer()\n",
    "for row in data.iterrows():\n",
    "    label = row[1]['label']\n",
    "    text = row[1]['text']\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = nltk.pos_tag(text)\n",
    "    text = [wnl.lemmatize(word, get_pos(pos)) for word, pos in text]\n",
    "    sentences.append(text)\n",
    "    labels.append(label)\n",
    "\n",
    "preprocessed_data = pd.DataFrame({\"label\":labels, \"text\":sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6fc53e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roman</td>\n",
       "      <td>[The, Roman, Empire, ,, the, ancient, empire, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>[The, Spanish, Empire, (, Spanish, :, Imperio,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russian</td>\n",
       "      <td>[The, Russian, Empire, ,, also, know, a, Imper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roman</td>\n",
       "      <td>[A, period, of, unrest, and, civil, war, in, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roman</td>\n",
       "      <td>[Augustus, establish, a, form, of, government,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0    Roman  [The, Roman, Empire, ,, the, ancient, empire, ...\n",
       "1  Spanish  [The, Spanish, Empire, (, Spanish, :, Imperio,...\n",
       "2  Russian  [The, Russian, Empire, ,, also, know, a, Imper...\n",
       "3    Roman  [A, period, of, unrest, and, civil, war, in, t...\n",
       "4    Roman  [Augustus, establish, a, form, of, government,..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16fa89",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5f74b9b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for label in preprocessed_data['label']:\n",
    "    labels.append(empires.index(label))\n",
    "preprocessed_data['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7f1fcf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[The, Roman, Empire, ,, the, ancient, empire, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[The, Spanish, Empire, (, Spanish, :, Imperio,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[The, Russian, Empire, ,, also, know, a, Imper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[A, period, of, unrest, and, civil, war, in, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[Augustus, establish, a, form, of, government,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  [The, Roman, Empire, ,, the, ancient, empire, ...\n",
       "1      1  [The, Spanish, Empire, (, Spanish, :, Imperio,...\n",
       "2      2  [The, Russian, Empire, ,, also, know, a, Imper...\n",
       "3      0  [A, period, of, unrest, and, civil, war, in, t...\n",
       "4      0  [Augustus, establish, a, form, of, government,..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09743410",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f925ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "text = []\n",
    "for sentence in preprocessed_data['text']:\n",
    "    text.append(\" \".join(sentence))\n",
    "vectorizer.fit(text)\n",
    "vectorized_text = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502c356",
   "metadata": {},
   "source": [
    "## Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f08fa455",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = set()\n",
    "for sentence in data['text']:\n",
    "    for word in sentence:\n",
    "        uniques.add(word)\n",
    "num_uniques = len(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0af671e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = vectorized_text\n",
    "labels = preprocessed_data['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(text, labels, train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0097ae",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a962d610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571428571428572"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(random_state=2, max_depth=150, max_features=1000)\n",
    "decision_tree.fit(x_train, y_train)\n",
    "decision_tree_score = decision_tree.score(x_test, y_test)\n",
    "decision_tree_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782ac3b",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c0847116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857142857142858"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0, n_estimators=300, max_depth=150, max_features=1000)\n",
    "random_forest.fit(x_train, y_train)\n",
    "random_forest_score = random_forest.score(x_test, y_test)\n",
    "random_forest_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a032c1",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b9251f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_c = 0.1\n",
    "best_kernel = \"linear\"\n",
    "support_vector_machine = SupportVectorClassifier(kernel=best_kernel, C=best_c)\n",
    "support_vector_machine.fit(x_train, y_train)\n",
    "support_vector_machine_score = support_vector_machine.score(x_test, y_test)\n",
    "support_vector_machine_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2560e5",
   "metadata": {},
   "source": [
    "### Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8895a",
   "metadata": {},
   "source": [
    "Sidenote: while one could convert x_Train from SparseTensor to array it causes the lstm model to have low accuracy and take 20 min per epoch, so its better to just format the data with tensorflow methods to convert it to a format that tensorflow is better equipped to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5b8cc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input example: [ 83  46  90  38  95   1 127 120  50  97  51  83  21  44 107  50 127 127\n",
      " 119   1 127  25  20 100  83  15  65  41  66  79  85 116 127 103  38  83\n",
      " 114 113  44  53  35  39  83  69  77 119  83  36 115  63  28  79  38 107\n",
      "  89  79  31  12  51  83 103 117  83  76  63  96  84  38  83  94 127  65\n",
      " 102  26 124  15  19  20 112  40  52   1  15  69 127 128  50  85  20  38\n",
      " 107  52  93  83  63  79 130  35  57  14 113  36  51  83 132  20  33  77\n",
      "  69  63  76 132  35  60  42 132  83  63  38  35  93  47  21  52  79  47\n",
      "  24  78  17  31  27 132  50 138 134   1  83  55  44  93 106  20  27  51\n",
      "  83  24  90  83   1  44  50 120  83  90  70  56  20 112  42  83  12  11\n",
      "  44  24 106  51  16 119  83  50 108  79  94  57  34   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0]\n",
      "label example: [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "text = preprocessed_data['text']\n",
    "labels = preprocessed_data['label']\n",
    "longest_sentence = max(text, key=lambda x: len(x))\n",
    "encoded_sentences = [one_hot(\" \".join(sentence), round(num_uniques*1.2)) for sentence in text]\n",
    "padded_sequences = pad_sequences(encoded_sentences, maxlen=len(longest_sentence), padding='post')\n",
    "\n",
    "def encode_labels(labels):\n",
    "    output = []\n",
    "    for label in labels:\n",
    "        label_array = np.zeros(len(empires))\n",
    "        label_array[label] = 1    \n",
    "        output.append(label_array)\n",
    "    return output\n",
    "\n",
    "labels = encode_labels(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=.33, random_state=463)\n",
    "print(\"input example:\", x_train[0])\n",
    "print(\"label example:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8b9ead97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696\n"
     ]
    }
   ],
   "source": [
    "print(len(padded_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ca1dcc00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Lstm(Model):\n",
    "    def __init__(self, uniques) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = Embedding(uniques, 64)\n",
    "        self.l2 = Bidirectional(LSTM(16))\n",
    "        self.l3 = Dropout(0.2)\n",
    "        self.l4 = Dense(128)\n",
    "        self.l6 = Dense(3, activation=\"sigmoid\")\n",
    "    \n",
    "    def call(self, inp):\n",
    "        x = self.l1(inp)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        x = mish(x)\n",
    "        x = self.l6(x)\n",
    "        return x\n",
    "\n",
    "lstm = Lstm(num_uniques)\n",
    "lstm.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "067a2374",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(num_uniques, 128))\n",
    "lstm.add(Bidirectional(LSTM(4)))\n",
    "lstm.add(Dropout(0.1))\n",
    "lstm.add(Dense(3, activation=\"sigmoid\"))\n",
    "lstm.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "882c282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(round(num_uniques*1.2), 128))\n",
    "lstm.add(Bidirectional(LSTM(32)))\n",
    "lstm.add(Dense(256, activation=\"relu\"))\n",
    "lstm.add(Dropout(0.5))\n",
    "lstm.add(Dense(128))\n",
    "lstm.add(Dense(3, activation=\"sigmoid\"))\n",
    "opt = Adam(learning_rate=0.00001)\n",
    "lstm.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7de356a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "233/233 [==============================] - 29s 124ms/step - loss: 0.3357 - accuracy: 0.8712 - val_loss: 0.8398 - val_accuracy: 0.7043\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - 30s 129ms/step - loss: 0.2653 - accuracy: 0.9056 - val_loss: 2.2787 - val_accuracy: 0.6652\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - 30s 129ms/step - loss: 0.3929 - accuracy: 0.8798 - val_loss: 0.9505 - val_accuracy: 0.5783\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - 30s 130ms/step - loss: 0.4638 - accuracy: 0.8348 - val_loss: 0.9087 - val_accuracy: 0.6957\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - 30s 130ms/step - loss: 0.2711 - accuracy: 0.9056 - val_loss: 1.1009 - val_accuracy: 0.6304\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - 30s 130ms/step - loss: 0.2710 - accuracy: 0.9077 - val_loss: 1.2548 - val_accuracy: 0.6652\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - 30s 130ms/step - loss: 0.1465 - accuracy: 0.9528 - val_loss: 2.5272 - val_accuracy: 0.6565\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - 30s 129ms/step - loss: 0.2952 - accuracy: 0.8927 - val_loss: 1.6142 - val_accuracy: 0.6609\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - 30s 129ms/step - loss: 0.1475 - accuracy: 0.9464 - val_loss: 1.5519 - val_accuracy: 0.6652\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - 30s 130ms/step - loss: 0.0760 - accuracy: 0.9785 - val_loss: 1.9978 - val_accuracy: 0.6565\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - 30s 130ms/step - loss: 0.3565 - accuracy: 0.8863 - val_loss: 1.2263 - val_accuracy: 0.6913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ed46e06050>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('best-weights.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
    "lstm.fit(np.array(x_train), np.array(y_train), epochs=100, verbose=1, batch_size=2, callbacks=[es, mc], validation_data=(x_test, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6a980155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 45ms/step - loss: 0.8565 - accuracy: 0.6478\n",
      "Test accuracy: 64.7826075553894\n"
     ]
    }
   ],
   "source": [
    "lstm_score = lstm.evaluate(x_test, np.array(y_test), verbose=1)\n",
    "lstm_acc = lstm_score[1]*100\n",
    "print(f'Test accuracy: {lstm_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "506c7cd5f11c76d07d3fb12eb0803be97b690f1666932904271a8a04a8fd81e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
